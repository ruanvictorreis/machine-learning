{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Mútipla com NumPy (Vetorizada)\n",
    "\n",
    "Universidade Federal de Campina Grande<br/>\n",
    "Programa de Pós-Graduação em Ciência da Computação<br/>\n",
    "Disciplina: Aprendizagem de Máquina<br/>\n",
    "Aluno: Ruan Victor Bertoldo Reis de Amorim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculando Residual Sum of Squares:\n",
    "$RSS(\\mathbf{w})=(y-{\\mathbf{H}\\mathbf{w}})^T(y-{\\mathbf{H}\\mathbf{w}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_rss_vectorized(w, X, Y):\n",
    "    res = Y - np.dot(X,w)\n",
    "    totalError = np.dot(res.T,res)\n",
    "    return totalError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculando Gradiente do RSS:\n",
    "$\\nabla RSS(\\mathbf{w})=-2{\\mathbf{H}^T}(y-{\\mathbf{H}\\mathbf{w}})$<br>\n",
    "\n",
    "### Gradiente Descentente - atualizando vetor de coeficientes:\n",
    "$\\mathbf{w}^{(t + 1)} = \\mathbf{w}^t - \\alpha \\nabla RSS(\\mathbf{w}^{(t)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_gradient_vectorized(w_current, X, Y, learningRate):\n",
    "    res =  Y - np.dot(X, w_current)\n",
    "    gradient = np.dot(-2*(X.T), res)\n",
    "    new_w = w_current - learningRate * gradient\n",
    "    return [new_w, gradient]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterações do algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent_runner_vectorized(starting_w, X, Y, learning_rate, epsilon):\n",
    "    w = starting_w\n",
    "    gradient = np.array([np.inf,np.inf, np.inf, np.inf, np.inf, np.inf])\n",
    "#    i = 0\n",
    "    while (np.linalg.norm(gradient)>=epsilon):\n",
    "        w, gradient = step_gradient_vectorized(w, X, Y, learning_rate)\n",
    "#        if i % 1000 == 0:\n",
    "#            print(\"MSE na iteração {0} é de {1}\".format(i, compute_mse_vectorized(w, X, Y)))\n",
    "#        i+= 1\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ponto inicial - Leitura dos Dados, Configurações e Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando gradiente descendente com w0 = [ 0.], w1 = [ 0.], w2 = [ 0.], w3 = [ 0.], w4 = [ 0.], w5 = [ 0.], error = [[ 4794.2359393]]\n",
      "Executando...\n",
      "\n",
      "Gradiente descendente convergiu com w0 = [ 1.69701235], w1 = [ 0.1037707], w2 = [ 0.04829923], w3 = [ 0.16400561], w4 = [ 0.38324509], w5 = [ 0.020779], error = [[ 36.1987279]]\n",
      "\n",
      "Tempo de Execução: 2288.2463932037354 ms\n"
     ]
    }
   ],
   "source": [
    "points = np.genfromtxt(\"sample_treino.csv\", delimiter=\",\")\n",
    "points = np.c_[np.ones(len(points)),points]\n",
    "X = points[:,[0, 1, 2, 3, 4, 5]]\n",
    "Y = points[:,6][:,np.newaxis]\n",
    "init_w = np.zeros((6,1))\n",
    "\n",
    "learning_rate = 0.00002\n",
    "epsilon = 0.05\n",
    "\n",
    "print(\"Iniciando gradiente descendente com w0 = {0}, w1 = {1}, w2 = {2}, w3 = {3}, w4 = {4}, w5 = {5}, error = {6}\"\n",
    "      .format(init_w[0], init_w[1], init_w[2], init_w[3], init_w[4], init_w[5], compute_rss_vectorized(init_w, X,Y)))\n",
    "print(\"Executando...\")\n",
    "\n",
    "tic = time.time()\n",
    "w = gradient_descent_runner_vectorized(init_w, X, Y, learning_rate, epsilon)\n",
    "toc = time.time()\n",
    "print(\"\\nGradiente descendente convergiu com w0 = {0}, w1 = {1}, w2 = {2}, w3 = {3}, w4 = {4}, w5 = {5}, error = {6}\".\n",
    "      format(w[0], w[1], w[2], w[3], w[4], w[5], compute_rss_vectorized(w,X,Y)))\n",
    "print(\"\\nTempo de Execução: \" + str(1000*(toc-tic)) + \" ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coeficientes da Regressão Linear do Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.10304143  0.0464367   0.16409834  0.38117843  0.02027816]]\n"
     ]
    }
   ],
   "source": [
    "ols = linear_model.LinearRegression()\n",
    "model = ols.fit(X, Y)\n",
    "print (model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### O algoritmo funcionou corretamente! Podemos ver que os valores dos coeficientes encontrados são muito próximos ao do Scikit Learn, com exceção do coeficiente w0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
